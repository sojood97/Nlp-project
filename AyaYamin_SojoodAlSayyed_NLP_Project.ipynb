{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Papers -- Split Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "titles = []\n",
    "texts = []\n",
    "abstracts=[]\n",
    "\n",
    "\n",
    "\n",
    "def read_papers(path):\n",
    "     for dirname, categoryname, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                if filename == 'README.TXT':\n",
    "                    filenames.remove(filename)\n",
    "                else:\n",
    "                    current_file = os.path.abspath(os.path.join(dirname, filename))\n",
    "                    open_file = open(current_file, 'r', encoding=\"utf16\",errors='ignore')\n",
    "                    text_data = open_file.read().split('\\n')\n",
    "                    text_data = list(filter(None, text_data))\n",
    "                    titles.append(text_data[0])\n",
    "                    abstracts.append(text_data[1])\n",
    "                    texts.append((dirname, filename, text_data[0],text_data[1], text_data[10:]))\n",
    "     df = pd.DataFrame(texts, columns=['Directory', 'FileName', 'Title','Summary', 'Text'])\n",
    "     df['Text'] = df.Text.astype(str)\n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Directory</th>\n",
       "      <th>FileName</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1-s2.0-S2211381912001087-main.txt</td>\n",
       "      <td>Available online at www.sciencedirect.com</td>\n",
       "      <td>Many modern service systems rely on a network ...</td>\n",
       "      <td>['While the ROASWSN design decision is to sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1012.2609.txt</td>\n",
       "      <td>J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...</td>\n",
       "      <td>Term weighting schemes often dominate the perf...</td>\n",
       "      <td>['*   This work was supported supported by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1207.1847v1.txt</td>\n",
       "      <td>The University of Sheﬃeld</td>\n",
       "      <td>The statistical methods derived and described ...</td>\n",
       "      <td>['Part 3.    Applications                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1209.3126v1.txt</td>\n",
       "      <td>Beyond Stemming and Lemmatization:</td>\n",
       "      <td>In Automatic Text Summarization, preprocessing...</td>\n",
       "      <td>['\\ufeff\\x0c', 'a heterogeneous set of documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1210.0852.txt</td>\n",
       "      <td>Detecting multiword phrases in mathematical te...</td>\n",
       "      <td>We present an approach for detecting multiword...</td>\n",
       "      <td>['•   locally symmetrical Finsler manifold(s) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1305.6143.txt</td>\n",
       "      <td>Fast and accurate sentiment classification usi...</td>\n",
       "      <td>Abstract. We have explored different methods o...</td>\n",
       "      <td>['We used a publicly available dataset of movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1306.6802v2.txt</td>\n",
       "      <td>Evaluation Measures for Hierarchical Classiﬁca...</td>\n",
       "      <td>Hierarchical classiﬁcation addresses the probl...</td>\n",
       "      <td>['1. It groups existing HC evaluation measures...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C:/Users/aya yamin/Downloads/papers/papers-tex...</td>\n",
       "      <td>1308.0850v5.txt</td>\n",
       "      <td>Generating Sequences With Recurrent Neural Net...</td>\n",
       "      <td>This paper shows how Long Short-term Memory re...</td>\n",
       "      <td>['Long Short-term Memory (LSTM) [16] is an RNN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Directory  \\\n",
       "0  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "1  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "2  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "3  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "4  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "5  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "6  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "7  C:/Users/aya yamin/Downloads/papers/papers-tex...   \n",
       "\n",
       "                            FileName  \\\n",
       "0  1-s2.0-S2211381912001087-main.txt   \n",
       "1                      1012.2609.txt   \n",
       "2                    1207.1847v1.txt   \n",
       "3                    1209.3126v1.txt   \n",
       "4                      1210.0852.txt   \n",
       "5                      1305.6143.txt   \n",
       "6                    1306.6802v2.txt   \n",
       "7                    1308.0850v5.txt   \n",
       "\n",
       "                                               Title  \\\n",
       "0          Available online at www.sciencedirect.com   \n",
       "1  J OURNAL OF I NFORMATION S CIENCE AND E NGINEE...   \n",
       "2                          The University of Sheﬃeld   \n",
       "3                 Beyond Stemming and Lemmatization:   \n",
       "4  Detecting multiword phrases in mathematical te...   \n",
       "5  Fast and accurate sentiment classification usi...   \n",
       "6  Evaluation Measures for Hierarchical Classiﬁca...   \n",
       "7  Generating Sequences With Recurrent Neural Net...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Many modern service systems rely on a network ...   \n",
       "1  Term weighting schemes often dominate the perf...   \n",
       "2  The statistical methods derived and described ...   \n",
       "3  In Automatic Text Summarization, preprocessing...   \n",
       "4  We present an approach for detecting multiword...   \n",
       "5  Abstract. We have explored different methods o...   \n",
       "6  Hierarchical classiﬁcation addresses the probl...   \n",
       "7  This paper shows how Long Short-term Memory re...   \n",
       "\n",
       "                                                Text  \n",
       "0  ['While the ROASWSN design decision is to sele...  \n",
       "1  ['*   This work was supported supported by the...  \n",
       "2  ['Part 3.    Applications                     ...  \n",
       "3  ['\\ufeff\\x0c', 'a heterogeneous set of documen...  \n",
       "4  ['•   locally symmetrical Finsler manifold(s) ...  \n",
       "5  ['We used a publicly available dataset of movi...  \n",
       "6  ['1. It groups existing HC evaluation measures...  \n",
       "7  ['Long Short-term Memory (LSTM) [16] is an RNN...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=read_papers('C:/Users/aya yamin/Downloads/papers/papers-text/Text Files/')\n",
    "\n",
    "data.head(n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map the contracted words with their intended meanings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What are contractions?\n",
    "\n",
    "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map = {\n",
    "                   \"ain't\": \"is not\", \n",
    "                   \"aren't\": \"are not\",\n",
    "                   \"can't\": \"cannot\", \n",
    "                   \"'cause\": \"because\",\n",
    "                   \"could've\": \"could have\",\n",
    "                   \"couldn't\": \"could not\",\n",
    "                   \"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \n",
    "                   \"don't\": \"do not\", \n",
    "                   \"hadn't\": \"had not\",\n",
    "                   \"hasn't\": \"has not\", \n",
    "                   \"haven't\": \"have not\",\n",
    "                   \"he'd\": \"he would\",\n",
    "                   \"he'll\": \"he will\",\n",
    "                   \"he's\": \"he is\", \n",
    "                   \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \n",
    "                   \"how'll\": \"how will\", \n",
    "                   \"how's\": \"how is\",\n",
    "                   \"I'd\": \"I would\", \n",
    "                   \"I'd've\": \"I would have\",\n",
    "                   \"I'll\": \"I will\",\n",
    "                   \"I'll've\": \"I will have\",\n",
    "                   \"I'm\": \"I am\",\n",
    "                   \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\",\n",
    "                   \"i'd've\": \"i would have\",\n",
    "                   \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\n",
    "                   \"i'm\": \"i am\", \n",
    "                   \"i've\": \"i have\",\n",
    "                   \"isn't\": \"is not\", \n",
    "                   \"it'd\": \"it would\",\n",
    "                   \"it'd've\": \"it would have\",\n",
    "                   \"it'll\": \"it will\",\n",
    "                   \"it'll've\": \"it will have\",\n",
    "                   \"it's\": \"it is\",\n",
    "                   \"let's\": \"let us\",\n",
    "                   \"ma'am\": \"madam\",\n",
    "                   \"mayn't\": \"may not\",\n",
    "                   \"might've\": \"might have\",\n",
    "                   \"mightn't\": \"might not\",\n",
    "                   \"mightn't've\": \"might not have\",\n",
    "                   \"must've\": \"must have\",\n",
    "                   \"mustn't\": \"must not\", \n",
    "                   \"mustn't've\": \"must not have\",\n",
    "                   \"needn't\": \"need not\", \n",
    "                   \"needn't've\": \"need not have\",\n",
    "                   \"o'clock\": \"of the clock\",\n",
    "                   \"oughtn't\": \"ought not\", \n",
    "                   \"oughtn't've\": \"ought not have\",\n",
    "                   \"shan't\": \"shall not\", \n",
    "                   \"sha'n't\": \"shall not\", \n",
    "                   \"shan't've\": \"shall not have\",\n",
    "                   \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\",\n",
    "                   \"she'll\": \"she will\",\n",
    "                   \"she'll've\": \"she will have\",\n",
    "                   \"she's\": \"she is\",\n",
    "                   \"should've\": \"should have\",\n",
    "                   \"shouldn't\": \"should not\",\n",
    "                   \"shouldn't've\": \"should not have\",\n",
    "                   \"so've\": \"so have\",\n",
    "                   \"so's\": \"so as\",\n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \n",
    "                   \"that'd've\": \"that would have\", \n",
    "                   \"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\",\n",
    "                   \"there'd've\": \"there would have\", \n",
    "                   \"there's\": \"there is\", \n",
    "                   \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\",\n",
    "                   \"they'd've\": \"they would have\",\n",
    "                   \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\",\n",
    "                   \"they're\": \"they are\",\n",
    "                   \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\",\n",
    "                   \"wasn't\": \"was not\",\n",
    "                   \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\",\n",
    "                   \"we'll\": \"we will\",\n",
    "                   \"we'll've\": \"we will have\",\n",
    "                   \"we're\": \"we are\",\n",
    "                   \"we've\": \"we have\", \n",
    "                   \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \n",
    "                   \"what'll've\": \"what will have\", \n",
    "                   \"what're\": \"what are\",\n",
    "                   \"what's\": \"what is\",\n",
    "                   \"what've\": \"what have\",\n",
    "                   \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \n",
    "                   \"where'd\": \"where did\", \n",
    "                   \"where's\": \"where is\",\n",
    "                   \"where've\": \"where have\", \n",
    "                   \"who'll\": \"who will\", \n",
    "                   \"who'll've\": \"who will have\",\n",
    "                   \"who's\": \"who is\", \n",
    "                   \"who've\": \"who have\",\n",
    "                   \"why's\": \"why is\",\n",
    "                   \"why've\": \"why have\", \n",
    "                   \"will've\": \"will have\", \n",
    "                   \"won't\": \"will not\",\n",
    "                   \"won't've\": \"will not have\",\n",
    "                   \"would've\": \"would have\",\n",
    "                   \"wouldn't\": \"would not\",\n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                   \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\n",
    "                   \"y'all're\": \"you all are\",\n",
    "                   \"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \n",
    "                   \"you'd've\": \"you would have\",\n",
    "                   \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\",\n",
    "                   \"you're\": \"you are\",\n",
    "                   \"you've\": \"you have\"\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clean the text and abstract using the contrcation_mapping and removing irrelevant letters and words. Finally,tokenize the sentences for further processing.\n",
    "1-Remove stop words\n",
    "2-Convert to lower case\n",
    "3-Replace digit number with space\n",
    "4-remove short words\n",
    "\n",
    "Apply preproessing function on paper text and abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re        \n",
    "import numpy as np  \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords   \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "def summary_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub('\"','', str(text))\n",
    "    newString = ' '.join([contraction_map[t] if t in contraction_map else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(summary_cleaner(t))\n",
    "    \n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(summary_cleaner(t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Drop nan value (nul value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roaswsn design decision select hub arcs flow c...</td>\n",
       "      <td>many modern service systems rely network hub f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work supported supported high tech program gra...</td>\n",
       "      <td>term weighting schemes often dominate performa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>part applications chapter collocation coocurre...</td>\n",
       "      <td>statistical methods derived described thesis p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ufeff heterogeneous set documents focused topi...</td>\n",
       "      <td>automatic text summarization preprocessing imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>locally symmetrical finsler manifold local equ...</td>\n",
       "      <td>present approach detecting multiword phrases m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>used publicly available dataset movie reviews ...</td>\n",
       "      <td>abstract explored different methods improving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>groups existing evaluation measures two main t...</td>\n",
       "      <td>hierarchical classi cation addresses problem c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long short term memory lstm rnn architecture d...</td>\n",
       "      <td>paper shows long short term memory recurrent n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  roaswsn design decision select hub arcs flow c...   \n",
       "1  work supported supported high tech program gra...   \n",
       "2  part applications chapter collocation coocurre...   \n",
       "3  ufeff heterogeneous set documents focused topi...   \n",
       "4  locally symmetrical finsler manifold local equ...   \n",
       "5  used publicly available dataset movie reviews ...   \n",
       "6  groups existing evaluation measures two main t...   \n",
       "7  long short term memory lstm rnn architecture d...   \n",
       "\n",
       "                                     cleaned_summary  \n",
       "0  many modern service systems rely network hub f...  \n",
       "1  term weighting schemes often dominate performa...  \n",
       "2  statistical methods derived described thesis p...  \n",
       "3  automatic text summarization preprocessing imp...  \n",
       "4  present approach detecting multiword phrases m...  \n",
       "5  abstract explored different methods improving ...  \n",
       "6  hierarchical classi cation addresses problem c...  \n",
       "7  paper shows long short term memory recurrent n...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "dataa = data.filter(['cleaned_text','cleaned_summary'], axis=1)\n",
    "dataa.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Into Train/Test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test size = 20%\n",
    "X= cleaned text(paper text after clean)\n",
    "y=cleaned summary (original abstract after clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(dataa['cleaned_text'],dataa['cleaned_summary'],test_size=0.2,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Distribution of the sequence of summary word count and cleaned text word count , also use this distribution \n",
    "to define maximum length text and maxiumam sumarry length text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in dataa['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in dataa['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the lengths for summary and text.\n",
    "max_len_text=500\n",
    "max_len_summary=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Text Tokenizer & Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1\n",
    "\n",
    "\n",
    "#preparing a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size  =   len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Build the model. Key components of the model are as follows:\n",
    "\n",
    "1- Encoder- Encoder_inputs is used in order to encode the words into numeric data for processing by LSTM layers.\n",
    "2-LSTM Layers- We use 3 LSTM layers in order to process the data effectively. You can also experiment by adding or removing the layers in order to find more better accuracies. return_sequences in LSTM layer is set true until we want to add more layers consecutively.\n",
    "3-Decoder- Decoder again converts the numeric data into the understandable word formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K \n",
    "K.clear_session() \n",
    "#\"latent_dim\" is the number of nodes used as input of the generator\n",
    "latent_dim = 500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attension "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adding the Attention layer: Attention layer is used to selectively choose the relevant information while discarding the non-useful information by cognitively mapping the generated sentences with the inputs of encoder layer.\n",
    "Dense Layer: It mathematically represents the matrix vector multiplication in neurons and is used to change the dimensions of the vectors for processing between various layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        return fake_state_c, fake_state_e\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 500, 500)     33412000    ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_24 (LSTM)                 [(None, 500, 500),   2002000     ['embedding_12[0][0]']           \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_25 (LSTM)                 [(None, 500, 500),   2002000     ['lstm_24[0][0]']                \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, None, 500)    3762000     ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_26 (LSTM)                 [(None, 500, 500),   2002000     ['lstm_25[0][0]']                \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " lstm_27 (LSTM)                 [(None, None, 500),  2002000     ['embedding_13[0][0]',           \n",
      "                                 (None, 500),                     'lstm_26[0][1]',                \n",
      "                                 (None, 500)]                     'lstm_26[0][2]']                \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, None, 7524)  3769524     ['lstm_27[0][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48,951,524\n",
      "Trainable params: 48,951,524\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "#Preparing LSTM layer 1 \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "#Preparing LSTM layer 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "#Preparing LSTM layer 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "# Decoder layer \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])    \n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "#Adding the dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_outputs) \n",
    "# Prepare the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "# Compiling the RNN model\n",
    "#Compilation\n",
    "#Before training a model, you need to configure the learning process,\n",
    "#which is done via the compile method. It receives three arguments:\n",
    "#Optimization is an important process which optimize the input weights by comparing the prediction and the loss function. (such as rmsprop or adagrad)\n",
    "#a loss function. This is the objective that the model will try to minimize Loss function is used to find error or deviation in the learning process. \n",
    "#Keras requires loss function during model compilation process.\n",
    "#Metrics is used to evaluate the performance of your model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy' ,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_14 (Embedding)       (None, 500, 500)     33412000    ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_28 (LSTM)                 [(None, 500, 500),   2002000     ['embedding_14[0][0]']           \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_29 (LSTM)                 [(None, 500, 500),   2002000     ['lstm_28[0][0]']                \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " embedding_15 (Embedding)       (None, None, 500)    3762000     ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_30 (LSTM)                 [(None, 500, 500),   2002000     ['lstm_29[0][0]']                \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " lstm_31 (LSTM)                 [(None, None, 500),  2002000     ['embedding_15[0][0]',           \n",
      "                                 (None, 500),                     'lstm_30[0][1]',                \n",
      "                                 (None, 500)]                     'lstm_30[0][2]']                \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, None, 7524)  3769524     ['lstm_31[0][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48,951,524\n",
      "Trainable params: 48,951,524\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model on a batch size of 512 and validate it on  10% of  dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_(ep,bs):\n",
    "    history = model.fit(\n",
    "    [x_tr, y_tr[:, :-1]],\n",
    "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
    "    epochs=ep,\n",
    "    batch_size=bs,\n",
    "    validation_data=([x_val, y_val[:, :-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]),\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 793s 56s/step - loss: 6.9817 - accuracy: 0.2099 - val_loss: 6.3174 - val_accuracy: 0.2051\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 774s 55s/step - loss: 6.1568 - accuracy: 0.2281 - val_loss: 6.0496 - val_accuracy: 0.2051\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 824s 59s/step - loss: 5.8525 - accuracy: 0.2296 - val_loss: 5.9580 - val_accuracy: 0.2129\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 818s 58s/step - loss: 5.7575 - accuracy: 0.2324 - val_loss: 5.9367 - val_accuracy: 0.2080\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 804s 58s/step - loss: 5.6893 - accuracy: 0.2327 - val_loss: 5.9304 - val_accuracy: 0.2112\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 788s 56s/step - loss: 5.6287 - accuracy: 0.2344 - val_loss: 5.9250 - val_accuracy: 0.2122\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 925s 67s/step - loss: 5.5769 - accuracy: 0.2364 - val_loss: 5.9432 - val_accuracy: 0.2101\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 1156s 83s/step - loss: 5.4976 - accuracy: 0.2396 - val_loss: 5.9399 - val_accuracy: 0.2179\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 1161s 83s/step - loss: 5.4306 - accuracy: 0.2413 - val_loss: 5.9381 - val_accuracy: 0.2174\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 911s 64s/step - loss: 5.3591 - accuracy: 0.2438 - val_loss: 5.9690 - val_accuracy: 0.2173\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 761s 54s/step - loss: 5.2967 - accuracy: 0.2476 - val_loss: 5.9562 - val_accuracy: 0.2225\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 748s 53s/step - loss: 5.2291 - accuracy: 0.2486 - val_loss: 5.9630 - val_accuracy: 0.2219\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 806s 58s/step - loss: 5.1694 - accuracy: 0.2509 - val_loss: 5.9741 - val_accuracy: 0.2215\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 904s 65s/step - loss: 5.1127 - accuracy: 0.2528 - val_loss: 5.9686 - val_accuracy: 0.2255\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 888s 64s/step - loss: 5.0491 - accuracy: 0.2555 - val_loss: 5.9657 - val_accuracy: 0.2258\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 896s 64s/step - loss: 4.9809 - accuracy: 0.2582 - val_loss: 5.9859 - val_accuracy: 0.2292\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 879s 63s/step - loss: 4.9054 - accuracy: 0.2612 - val_loss: 5.9823 - val_accuracy: 0.2304\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 857s 61s/step - loss: 4.8293 - accuracy: 0.2651 - val_loss: 5.9928 - val_accuracy: 0.2331\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 749s 53s/step - loss: 4.7629 - accuracy: 0.2690 - val_loss: 5.9923 - val_accuracy: 0.2379\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 4.6850 - accuracy: 0.2732 - val_loss: 5.9879 - val_accuracy: 0.2384\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 752s 54s/step - loss: 4.6094 - accuracy: 0.2759 - val_loss: 6.0004 - val_accuracy: 0.2405\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 749s 53s/step - loss: 4.5333 - accuracy: 0.2813 - val_loss: 6.0039 - val_accuracy: 0.2432\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 4.4509 - accuracy: 0.2852 - val_loss: 6.0037 - val_accuracy: 0.2441\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 742s 53s/step - loss: 4.3586 - accuracy: 0.2913 - val_loss: 6.0045 - val_accuracy: 0.2465\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 4.2952 - accuracy: 0.2974 - val_loss: 6.0130 - val_accuracy: 0.2482\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 754s 54s/step - loss: 4.1981 - accuracy: 0.3057 - val_loss: 6.0222 - val_accuracy: 0.2505\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 4.1204 - accuracy: 0.3131 - val_loss: 6.0211 - val_accuracy: 0.2506\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 4.0369 - accuracy: 0.3212 - val_loss: 6.0277 - val_accuracy: 0.2503\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 747s 53s/step - loss: 3.9501 - accuracy: 0.3321 - val_loss: 6.0557 - val_accuracy: 0.2530\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 745s 53s/step - loss: 3.8734 - accuracy: 0.3412 - val_loss: 6.0365 - val_accuracy: 0.2539\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 3.7840 - accuracy: 0.3511 - val_loss: 6.0502 - val_accuracy: 0.2526\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 3.7042 - accuracy: 0.3614 - val_loss: 6.0687 - val_accuracy: 0.2547\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 3.6117 - accuracy: 0.3745 - val_loss: 6.0742 - val_accuracy: 0.2553\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 3.5330 - accuracy: 0.3854 - val_loss: 6.1005 - val_accuracy: 0.2560\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 3.4448 - accuracy: 0.4011 - val_loss: 6.0984 - val_accuracy: 0.2578\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 745s 53s/step - loss: 3.3632 - accuracy: 0.4115 - val_loss: 6.1297 - val_accuracy: 0.2586\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 747s 53s/step - loss: 3.2848 - accuracy: 0.4238 - val_loss: 6.1321 - val_accuracy: 0.2609\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 3.1950 - accuracy: 0.4374 - val_loss: 6.1344 - val_accuracy: 0.2630\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 3.1154 - accuracy: 0.4518 - val_loss: 6.1487 - val_accuracy: 0.2612\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 749s 53s/step - loss: 3.0339 - accuracy: 0.4663 - val_loss: 6.1613 - val_accuracy: 0.2624\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 755s 53s/step - loss: 2.9566 - accuracy: 0.4760 - val_loss: 6.1768 - val_accuracy: 0.2620\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 745s 53s/step - loss: 2.8647 - accuracy: 0.4950 - val_loss: 6.1913 - val_accuracy: 0.2634\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 2.8019 - accuracy: 0.5024 - val_loss: 6.2046 - val_accuracy: 0.2616\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 2.7206 - accuracy: 0.5169 - val_loss: 6.2412 - val_accuracy: 0.2631\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 2.6644 - accuracy: 0.5267 - val_loss: 6.2280 - val_accuracy: 0.2638\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 743s 53s/step - loss: 2.5683 - accuracy: 0.5445 - val_loss: 6.2399 - val_accuracy: 0.2669\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 745s 53s/step - loss: 2.5034 - accuracy: 0.5547 - val_loss: 6.2680 - val_accuracy: 0.2660\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 744s 53s/step - loss: 2.4074 - accuracy: 0.5705 - val_loss: 6.2831 - val_accuracy: 0.2667\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 753s 54s/step - loss: 2.3530 - accuracy: 0.5812 - val_loss: 6.3005 - val_accuracy: 0.2675\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 746s 53s/step - loss: 2.2513 - accuracy: 0.6000 - val_loss: 6.3251 - val_accuracy: 0.2661\n"
     ]
    }
   ],
   "source": [
    "history=Training_(50,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing Loss\n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing Accuracy \n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['accuracy'], label='train') \n",
    "pyplot.plot(history.history['val_accuracy'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 192s 7s/step - loss: 2.2165 - accuracy: 0.6054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2164688110351562, 0.605401873588562]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x_tr, y_tr[:, :-1]],y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dictionary to convert the index to word for target and source vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,state_h, state_c])\n",
    "encoder_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text, latent_dim))\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],[decoder_outputs2] + [state_h2, state_c2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"base_model.h5\")\n",
    "encoder_model.save(\"encoder_model.h5\")\n",
    "decoder_model.save(\"decoder_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
    "                + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'end':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'end' or len(decoded_sentence.split()) \\\n",
    "            >= max_len_summary - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert an integer sequence to a word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the seq2seq summary\n",
    "\n",
    "def seq2seqsummary(input_sequence):\n",
    "    newString=''\n",
    "    for i in input_sequence:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "\n",
    "# To convert sequence to text\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original Text: complex main acm users wise improving fails visualization phoneme unsupervised complicated required suggests usunier arxiv desired adam recurrent account error arxiv desired adam sequence reading unsupervised approaches matrix sentence generation rate iii taken replaced since common ufeff blocks recognition get arxiv desired training extractive ing sphinx loss learning improve sentence \n",
      "\n",
      "\n",
      "1/1 [==============================] - 1s 694ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted summary:  observe gertz closely callhome responding wrapper reg reg reg kurohashi kurohashi restrict restrict propensity simon guo schema uninterpretability cide paragraphs recommendation polarization would rather milano annotating asked frontend oscar cldnn roughly ports vdcnn vdcnn collectively collectively manufacturing speed communicate succession ablation mixing bahdanau hinese personality personality contiguous trellis supervision\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for i in range(len(x_val)):\n",
    "#print(\"Text:\",seq2text(x_tr[0]))\n",
    "print(\"\\n\")\n",
    "print(\"Original Text:\",seq2text(y_val[0]))\n",
    "  #\n",
    "  #print(\"Original summary:\",seq2seqsummary(y_val[i]))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted summary:\",decode_sequence(x_val[0].reshape(1,max_len_text)))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 678ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\n",
      "\n",
      "[{'rouge-1': {'r': 0.0031847133757961785, 'p': 0.02857142857142857, 'f': 0.0057306572212056485}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0031847133757961785, 'p': 0.02857142857142857, 'f': 0.0057306572212056485}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "#for i in range(len(x_val)):\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores( decode_sequence(x_val[50].reshape(1,max_len_text)), seq2text(x_tr[50]))\n",
    "#print(\"for paper \" %{i})\n",
    "print(\"\\n\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 688ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Individual 1-gram: 0.063131\n",
      "1/1 [==============================] - 1s 680ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Individual 2-gram: 0.000000\n",
      "1/1 [==============================] - 1s 667ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Individual 3-gram: 0.000000\n",
      "1/1 [==============================] - 1s 677ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Individual 4-gram: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "#for i in range(len(x_val)):\n",
    "#score = sentence_bleu(seq2text(y_val[5]), decode_sequence(x_val[5].reshape(1,max_len_text)))\n",
    "#print(score)\n",
    "\n",
    "#BLEU Score of Training set\n",
    "#n-gram individual BLEU\n",
    "\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(seq2text(y_val[10]), decode_sequence(x_val[10].reshape(1,max_len_text)), weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(seq2text(y_val[10]), decode_sequence(x_val[10].reshape(1,max_len_text)), weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(seq2text(y_val[10]), decode_sequence(x_val[10].reshape(1,max_len_text)), weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(seq2text(y_val[10]), decode_sequence(x_val[10].reshape(1,max_len_text)), weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 703ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "9.250907394577878e-232\n"
     ]
    }
   ],
   "source": [
    "score = sentence_bleu(seq2text(y_val[5]), decode_sequence(x_val[5].reshape(1,max_len_text)))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
